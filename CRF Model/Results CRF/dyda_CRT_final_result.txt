Namespace(corpus='dyda', mode='train', nclass=4, batch_size=10, batch_size_val=10, emb_batch=0, epochs=100, gpu='0,1', lr=0.0001, nlayer=2, chunk_size=0, dropout=0.5, speaker_info='emb_cls', topic_info='emb_cls', nfinetune=1, seed=0)
Tokenizing train....
Done
Tokenizing val....
Done
Tokenizing test....
Done
Done

Let's use 2 GPUs!
Initializing model....
********************Epoch: 1********************
Batch: 1/1112	loss: 27.798
Batch: 56/1112	loss: 18.904
Batch: 111/1112	loss: 14.090
Batch: 166/1112	loss: 18.867
Batch: 221/1112	loss: 10.033
Batch: 276/1112	loss: 8.213
Batch: 331/1112	loss: 4.797
Batch: 386/1112	loss: 9.572
Batch: 441/1112	loss: 3.362
Batch: 496/1112	loss: 2.205
Batch: 551/1112	loss: 5.333
Batch: 606/1112	loss: 6.594
Batch: 661/1112	loss: 4.686
Batch: 716/1112	loss: 8.545
Batch: 771/1112	loss: 5.131
Batch: 826/1112	loss: 11.742
Batch: 881/1112	loss: 3.114
Batch: 936/1112	loss: 4.875
Batch: 991/1112	loss: 5.362
Batch: 1046/1112	loss: 7.944
Batch: 1101/1112	loss: 3.270
Batch: 1112/1112	loss: 3.327
Epoch 1	Train Loss: 8.585	Val Acc: 0.811	Test Acc: 0.840
Best Epoch: 1	Best Epoch Val Acc: 0.811	Best Epoch Test Acc: 0.840, Best Test Acc: 0.840

********************Epoch: 2********************
Batch: 1/1112	loss: 5.089
Batch: 56/1112	loss: 5.844
Batch: 111/1112	loss: 5.730
Batch: 166/1112	loss: 6.427
Batch: 221/1112	loss: 9.373
Batch: 276/1112	loss: 7.801
Batch: 331/1112	loss: 4.116
Batch: 386/1112	loss: 3.014
Batch: 441/1112	loss: 4.512
Batch: 496/1112	loss: 10.125
Batch: 551/1112	loss: 7.052
Batch: 606/1112	loss: 6.028
Batch: 661/1112	loss: 4.965
Batch: 716/1112	loss: 5.213
Batch: 771/1112	loss: 6.376
Batch: 826/1112	loss: 4.488
Batch: 881/1112	loss: 2.780
Batch: 936/1112	loss: 7.488
Batch: 991/1112	loss: 4.879
Batch: 1046/1112	loss: 5.334
Batch: 1101/1112	loss: 2.486
Batch: 1112/1112	loss: 6.767
Epoch 2	Train Loss: 6.564	Val Acc: 0.809	Test Acc: 0.840
Best Epoch: 1	Best Epoch Val Acc: 0.811	Best Epoch Test Acc: 0.840, Best Test Acc: 0.840

********************Epoch: 3********************
Batch: 1/1112	loss: 14.055
Batch: 56/1112	loss: 5.063
Batch: 111/1112	loss: 5.874
Batch: 166/1112	loss: 5.650
Batch: 221/1112	loss: 5.409
Batch: 276/1112	loss: 11.024
Batch: 331/1112	loss: 8.079
Batch: 386/1112	loss: 3.695
Batch: 441/1112	loss: 2.575
Batch: 496/1112	loss: 8.100
Batch: 551/1112	loss: 5.012
Batch: 606/1112	loss: 7.427
Batch: 661/1112	loss: 3.529
Batch: 716/1112	loss: 4.776
Batch: 771/1112	loss: 6.996
Batch: 826/1112	loss: 5.597
Batch: 881/1112	loss: 4.779
Batch: 936/1112	loss: 4.309
Batch: 991/1112	loss: 5.327
Batch: 1046/1112	loss: 9.693
Batch: 1101/1112	loss: 4.620
Batch: 1112/1112	loss: 5.734
Epoch 3	Train Loss: 6.067	Val Acc: 0.831	Test Acc: 0.856
Best Epoch: 3	Best Epoch Val Acc: 0.831	Best Epoch Test Acc: 0.856, Best Test Acc: 0.856

********************Epoch: 4********************
Batch: 1/1112	loss: 3.260
Batch: 56/1112	loss: 9.596
Batch: 111/1112	loss: 5.335
Batch: 166/1112	loss: 2.384
Batch: 221/1112	loss: 6.573
Batch: 276/1112	loss: 5.440
Batch: 331/1112	loss: 5.714
Batch: 386/1112	loss: 4.784
Batch: 441/1112	loss: 2.598
Batch: 496/1112	loss: 6.134
Batch: 551/1112	loss: 3.371
Batch: 606/1112	loss: 6.564
Batch: 661/1112	loss: 6.654
Batch: 716/1112	loss: 7.618
Batch: 771/1112	loss: 5.676
Batch: 826/1112	loss: 2.948
Batch: 881/1112	loss: 5.416
Batch: 936/1112	loss: 6.225
Batch: 991/1112	loss: 4.224
Batch: 1046/1112	loss: 3.941
Batch: 1101/1112	loss: 2.711
Batch: 1112/1112	loss: 8.043
Epoch 4	Train Loss: 5.693	Val Acc: 0.831	Test Acc: 0.849
Best Epoch: 3	Best Epoch Val Acc: 0.831	Best Epoch Test Acc: 0.856, Best Test Acc: 0.856

********************Epoch: 5********************
Batch: 1/1112	loss: 5.299
Batch: 56/1112	loss: 5.426
Batch: 111/1112	loss: 6.234
Batch: 166/1112	loss: 3.958
Batch: 221/1112	loss: 2.247
Batch: 276/1112	loss: 3.644
Batch: 331/1112	loss: 7.582
Batch: 386/1112	loss: 2.536
Batch: 441/1112	loss: 7.556
Batch: 496/1112	loss: 4.278
Batch: 551/1112	loss: 4.776
Batch: 606/1112	loss: 6.571
Batch: 661/1112	loss: 10.855
Batch: 716/1112	loss: 3.658
Batch: 771/1112	loss: 3.273
Batch: 826/1112	loss: 3.687
Batch: 881/1112	loss: 8.735
Batch: 936/1112	loss: 4.274
Batch: 991/1112	loss: 9.383
Batch: 1046/1112	loss: 3.546
Batch: 1101/1112	loss: 6.274
Batch: 1112/1112	loss: 6.494
Epoch 5	Train Loss: 5.430	Val Acc: 0.835	Test Acc: 0.860
Best Epoch: 5	Best Epoch Val Acc: 0.835	Best Epoch Test Acc: 0.860, Best Test Acc: 0.860

********************Epoch: 6********************
Batch: 1/1112	loss: 8.622
Batch: 56/1112	loss: 5.109
Batch: 111/1112	loss: 6.369
Batch: 166/1112	loss: 3.842
Batch: 221/1112	loss: 5.245
Batch: 276/1112	loss: 4.491
Batch: 331/1112	loss: 5.621
Batch: 386/1112	loss: 11.608
Batch: 441/1112	loss: 2.263
Batch: 496/1112	loss: 3.617
Batch: 551/1112	loss: 5.111
Batch: 606/1112	loss: 3.274
Batch: 661/1112	loss: 3.853
Batch: 716/1112	loss: 6.980
Batch: 771/1112	loss: 3.384
Batch: 826/1112	loss: 5.877
Batch: 881/1112	loss: 3.910
Batch: 936/1112	loss: 6.735
Batch: 991/1112	loss: 3.573
Batch: 1046/1112	loss: 3.419
Batch: 1101/1112	loss: 2.024
Batch: 1112/1112	loss: 4.207
Epoch 6	Train Loss: 5.190	Val Acc: 0.831	Test Acc: 0.857
Best Epoch: 5	Best Epoch Val Acc: 0.835	Best Epoch Test Acc: 0.860, Best Test Acc: 0.860

********************Epoch: 7********************
Batch: 1/1112	loss: 7.063
Batch: 56/1112	loss: 3.936
Batch: 111/1112	loss: 2.518
Batch: 166/1112	loss: 6.386
Batch: 221/1112	loss: 3.563
Batch: 276/1112	loss: 4.405
Batch: 331/1112	loss: 2.076
Batch: 386/1112	loss: 9.739
Batch: 441/1112	loss: 3.851
Batch: 496/1112	loss: 4.151
Batch: 551/1112	loss: 4.120
Batch: 606/1112	loss: 5.626
Batch: 661/1112	loss: 2.421
Batch: 716/1112	loss: 4.032
Batch: 771/1112	loss: 6.207
Batch: 826/1112	loss: 8.231
Batch: 881/1112	loss: 5.710
Batch: 936/1112	loss: 2.591
Batch: 991/1112	loss: 3.498
Batch: 1046/1112	loss: 3.399
Batch: 1101/1112	loss: 4.859
Batch: 1112/1112	loss: 3.550
Epoch 7	Train Loss: 4.934	Val Acc: 0.838	Test Acc: 0.862
Best Epoch: 7	Best Epoch Val Acc: 0.838	Best Epoch Test Acc: 0.862, Best Test Acc: 0.862

********************Epoch: 8********************
Batch: 1/1112	loss: 3.512
Batch: 56/1112	loss: 6.352
Batch: 111/1112	loss: 5.086
Batch: 166/1112	loss: 5.884
Batch: 221/1112	loss: 3.629
Batch: 276/1112	loss: 8.564
Batch: 331/1112	loss: 2.375
Batch: 386/1112	loss: 5.684
Batch: 441/1112	loss: 7.018
Batch: 496/1112	loss: 4.814
Batch: 551/1112	loss: 5.953
Batch: 606/1112	loss: 4.321
Batch: 661/1112	loss: 5.371
Batch: 716/1112	loss: 2.702
Batch: 771/1112	loss: 5.991
Batch: 826/1112	loss: 3.268
Batch: 881/1112	loss: 3.451
Batch: 936/1112	loss: 1.766
Batch: 991/1112	loss: 4.841
Batch: 1046/1112	loss: 8.732
Batch: 1101/1112	loss: 3.839
Batch: 1112/1112	loss: 4.803
Epoch 8	Train Loss: 4.755	Val Acc: 0.840	Test Acc: 0.861
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.862

********************Epoch: 9********************
Batch: 1/1112	loss: 5.233
Batch: 56/1112	loss: 4.418
Batch: 111/1112	loss: 4.130
Batch: 166/1112	loss: 2.376
Batch: 221/1112	loss: 5.166
Batch: 276/1112	loss: 4.097
Batch: 331/1112	loss: 3.892
Batch: 386/1112	loss: 1.658
Batch: 441/1112	loss: 10.145
Batch: 496/1112	loss: 5.134
Batch: 551/1112	loss: 2.661
Batch: 606/1112	loss: 7.354
Batch: 661/1112	loss: 2.223
Batch: 716/1112	loss: 5.561
Batch: 771/1112	loss: 6.326
Batch: 826/1112	loss: 5.620
Batch: 881/1112	loss: 2.232
Batch: 936/1112	loss: 3.847
Batch: 991/1112	loss: 4.611
Batch: 1046/1112	loss: 4.457
Batch: 1101/1112	loss: 11.959
Batch: 1112/1112	loss: 4.562
Epoch 9	Train Loss: 4.525	Val Acc: 0.837	Test Acc: 0.855
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.862

********************Epoch: 10********************
Batch: 1/1112	loss: 7.424
Batch: 56/1112	loss: 3.801
Batch: 111/1112	loss: 9.690
Batch: 166/1112	loss: 2.099
Batch: 221/1112	loss: 3.793
Batch: 276/1112	loss: 3.184
Batch: 331/1112	loss: 2.475
Batch: 386/1112	loss: 5.039
Batch: 441/1112	loss: 2.924
Batch: 496/1112	loss: 3.901
Batch: 551/1112	loss: 3.352
Batch: 606/1112	loss: 3.708
Batch: 661/1112	loss: 6.115
Batch: 716/1112	loss: 5.643
Batch: 771/1112	loss: 5.283
Batch: 826/1112	loss: 6.060
Batch: 881/1112	loss: 3.139
Batch: 936/1112	loss: 4.109
Batch: 991/1112	loss: 5.228
Batch: 1046/1112	loss: 7.481
Batch: 1101/1112	loss: 5.051
Batch: 1112/1112	loss: 1.483
Epoch 10	Train Loss: 4.349	Val Acc: 0.837	Test Acc: 0.858
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.862

********************Epoch: 11********************
Batch: 1/1112	loss: 2.126
Batch: 56/1112	loss: 3.304
Batch: 111/1112	loss: 6.076
Batch: 166/1112	loss: 5.087
Batch: 221/1112	loss: 5.015
Batch: 276/1112	loss: 4.655
Batch: 331/1112	loss: 6.214
Batch: 386/1112	loss: 2.654
Batch: 441/1112	loss: 2.533
Batch: 496/1112	loss: 3.069
Batch: 551/1112	loss: 5.706
Batch: 606/1112	loss: 6.224
Batch: 661/1112	loss: 2.946
Batch: 716/1112	loss: 2.848
Batch: 771/1112	loss: 3.909
Batch: 826/1112	loss: 2.280
Batch: 881/1112	loss: 4.098
Batch: 936/1112	loss: 3.878
Batch: 991/1112	loss: 3.560
Batch: 1046/1112	loss: 2.244
Batch: 1101/1112	loss: 1.776
Batch: 1112/1112	loss: 2.600
Epoch 11	Train Loss: 4.138	Val Acc: 0.839	Test Acc: 0.865
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 12********************
Batch: 1/1112	loss: 3.702
Batch: 56/1112	loss: 4.840
Batch: 111/1112	loss: 1.972
Batch: 166/1112	loss: 5.678
Batch: 221/1112	loss: 2.550
Batch: 276/1112	loss: 5.607
Batch: 331/1112	loss: 1.932
Batch: 386/1112	loss: 2.250
Batch: 441/1112	loss: 2.046
Batch: 496/1112	loss: 3.165
Batch: 551/1112	loss: 7.323
Batch: 606/1112	loss: 3.144
Batch: 661/1112	loss: 3.024
Batch: 716/1112	loss: 3.879
Batch: 771/1112	loss: 4.431
Batch: 826/1112	loss: 2.310
Batch: 881/1112	loss: 6.145
Batch: 936/1112	loss: 3.888
Batch: 991/1112	loss: 4.808
Batch: 1046/1112	loss: 4.108
Batch: 1101/1112	loss: 7.320
Batch: 1112/1112	loss: 4.421
Epoch 12	Train Loss: 3.997	Val Acc: 0.839	Test Acc: 0.859
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 13********************
Batch: 1/1112	loss: 3.575
Batch: 56/1112	loss: 6.800
Batch: 111/1112	loss: 4.266
Batch: 166/1112	loss: 3.118
Batch: 221/1112	loss: 1.838
Batch: 276/1112	loss: 2.170
Batch: 331/1112	loss: 2.788
Batch: 386/1112	loss: 3.928
Batch: 441/1112	loss: 3.976
Batch: 496/1112	loss: 4.136
Batch: 551/1112	loss: 4.334
Batch: 606/1112	loss: 2.197
Batch: 661/1112	loss: 2.460
Batch: 716/1112	loss: 4.595
Batch: 771/1112	loss: 3.344
Batch: 826/1112	loss: 4.485
Batch: 881/1112	loss: 3.563
Batch: 936/1112	loss: 2.673
Batch: 991/1112	loss: 4.512
Batch: 1046/1112	loss: 3.290
Batch: 1101/1112	loss: 4.982
Batch: 1112/1112	loss: 6.972
Epoch 13	Train Loss: 3.789	Val Acc: 0.838	Test Acc: 0.859
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 14********************
Batch: 1/1112	loss: 1.694
Batch: 56/1112	loss: 3.579
Batch: 111/1112	loss: 1.277
Batch: 166/1112	loss: 2.026
Batch: 221/1112	loss: 6.568
Batch: 276/1112	loss: 1.050
Batch: 331/1112	loss: 2.102
Batch: 386/1112	loss: 5.511
Batch: 441/1112	loss: 4.380
Batch: 496/1112	loss: 4.652
Batch: 551/1112	loss: 1.848
Batch: 606/1112	loss: 2.637
Batch: 661/1112	loss: 5.956
Batch: 716/1112	loss: 3.492
Batch: 771/1112	loss: 3.365
Batch: 826/1112	loss: 4.973
Batch: 881/1112	loss: 2.616
Batch: 936/1112	loss: 3.746
Batch: 991/1112	loss: 1.556
Batch: 1046/1112	loss: 5.254
Batch: 1101/1112	loss: 2.425
Batch: 1112/1112	loss: 1.823
Epoch 14	Train Loss: 3.614	Val Acc: 0.839	Test Acc: 0.862
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 15********************
Batch: 1/1112	loss: 1.628
Batch: 56/1112	loss: 3.771
Batch: 111/1112	loss: 2.817
Batch: 166/1112	loss: 1.808
Batch: 221/1112	loss: 3.136
Batch: 276/1112	loss: 2.210
Batch: 331/1112	loss: 4.654
Batch: 386/1112	loss: 4.393
Batch: 441/1112	loss: 3.961
Batch: 496/1112	loss: 1.562
Batch: 551/1112	loss: 3.221
Batch: 606/1112	loss: 6.821
Batch: 661/1112	loss: 2.676
Batch: 716/1112	loss: 2.726
Batch: 771/1112	loss: 4.633
Batch: 826/1112	loss: 2.443
Batch: 881/1112	loss: 3.484
Batch: 936/1112	loss: 1.717
Batch: 991/1112	loss: 1.703
Batch: 1046/1112	loss: 4.057
Batch: 1101/1112	loss: 1.752
Batch: 1112/1112	loss: 2.916
Epoch 15	Train Loss: 3.478	Val Acc: 0.832	Test Acc: 0.861
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 16********************
Batch: 1/1112	loss: 3.360
Batch: 56/1112	loss: 2.240
Batch: 111/1112	loss: 2.453
Batch: 166/1112	loss: 2.684
Batch: 221/1112	loss: 2.303
Batch: 276/1112	loss: 2.903
Batch: 331/1112	loss: 1.521
Batch: 386/1112	loss: 1.143
Batch: 441/1112	loss: 3.000
Batch: 496/1112	loss: 2.753
Batch: 551/1112	loss: 5.565
Batch: 606/1112	loss: 4.561
Batch: 661/1112	loss: 2.993
Batch: 716/1112	loss: 1.520
Batch: 771/1112	loss: 4.617
Batch: 826/1112	loss: 4.494
Batch: 881/1112	loss: 2.919
Batch: 936/1112	loss: 4.638
Batch: 991/1112	loss: 3.954
Batch: 1046/1112	loss: 1.926
Batch: 1101/1112	loss: 0.861
Batch: 1112/1112	loss: 2.587
Epoch 16	Train Loss: 3.302	Val Acc: 0.835	Test Acc: 0.858
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 17********************
Batch: 1/1112	loss: 2.399
Batch: 56/1112	loss: 1.125
Batch: 111/1112	loss: 2.642
Batch: 166/1112	loss: 3.082
Batch: 221/1112	loss: 1.790
Batch: 276/1112	loss: 5.050
Batch: 331/1112	loss: 3.433
Batch: 386/1112	loss: 2.711
Batch: 441/1112	loss: 1.463
Batch: 496/1112	loss: 0.911
Batch: 551/1112	loss: 1.108
Batch: 606/1112	loss: 1.692
Batch: 661/1112	loss: 6.920
Batch: 716/1112	loss: 2.552
Batch: 771/1112	loss: 0.900
Batch: 826/1112	loss: 1.683
Batch: 881/1112	loss: 5.354
Batch: 936/1112	loss: 2.167
Batch: 991/1112	loss: 2.335
Batch: 1046/1112	loss: 2.789
Batch: 1101/1112	loss: 1.280
Batch: 1112/1112	loss: 3.579
Epoch 17	Train Loss: 3.129	Val Acc: 0.832	Test Acc: 0.853
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

********************Epoch: 18********************
Batch: 1/1112	loss: 2.255
Batch: 56/1112	loss: 2.617
Batch: 111/1112	loss: 2.943
Batch: 166/1112	loss: 2.279
Batch: 221/1112	loss: 2.207
Batch: 276/1112	loss: 5.065
Batch: 331/1112	loss: 3.576
Batch: 386/1112	loss: 2.238
Batch: 441/1112	loss: 1.953
Batch: 496/1112	loss: 0.832
Batch: 551/1112	loss: 3.064
Batch: 606/1112	loss: 3.756
Batch: 661/1112	loss: 1.591
Batch: 716/1112	loss: 2.087
Batch: 771/1112	loss: 6.913
Batch: 826/1112	loss: 2.432
Batch: 881/1112	loss: 2.754
Batch: 936/1112	loss: 2.315
Batch: 991/1112	loss: 3.082
Batch: 1046/1112	loss: 2.014
Batch: 1101/1112	loss: 1.303
Batch: 1112/1112	loss: 1.409
Epoch 18	Train Loss: 2.984	Val Acc: 0.840	Test Acc: 0.863
Best Epoch: 8	Best Epoch Val Acc: 0.840	Best Epoch Test Acc: 0.861, Best Test Acc: 0.865

Saving the best checkpoint....
Test Acc: 0.861
python -u engine.py --corpus=dyda --mode=train --gpu=0,1 --batch_size=10 --batch_size_val=10 --epochs=100 --lr=0.0001 --nlayer=2 --chunk_size=0 --dropout=0.5 --nfinetune=1  --speaker_info=emb_cls --topic_info=emb_cls --nclass=4 --emb_batch=0
